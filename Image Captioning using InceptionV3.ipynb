{"cells":[{"cell_type":"markdown","metadata":{"id":"bwDDl1PuJnBp"},"source":["**Task: given image and caption find probability by which caption belongs to image.** i.e. output the probability."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27249,"status":"ok","timestamp":1651842759517,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"og4Pc3hpiYTL","outputId":"aa7b179f-c1cc-41dc-f703-fadd205452b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data_root = '/content/drive/My Drive/IITB Courses/CS 772/Project/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"SfFBDfT9gPf3","outputId":"11f93e34-66ab-4731-bb46-c2618a4b6eec"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-04-07 21:07:47--  http://images.cocodataset.org/zips/train2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.141.201\n","Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.141.201|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19336861798 (18G) [application/zip]\n","Saving to: ‘/content/drive/My Drive/IITB Courses/CS 772/Project/data/train2017.zip’\n","\n","train2017.zip       100%[===================>]  18.01G  46.2MB/s    in 6m 12s  \n","\n","2022-04-07 21:13:59 (49.6 MB/s) - ‘/content/drive/My Drive/IITB Courses/CS 772/Project/data/train2017.zip’ saved [19336861798/19336861798]\n","\n"]}],"source":["# import os\n","# import zipfile\n","\n","# !wget http://images.cocodataset.org/zips/train2017.zip -P '/content/drive/My Drive/IITB Courses/CS 772/Project/data'\n","# with zipfile.ZipFile(os.path.join(data_root,'train2017.zip'), 'r') as zip_ref:\n","#     zip_ref.extractall(data_root)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9r2hU7v_m4Rp"},"outputs":[],"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import string\n","import os\n","import json\n","import math\n","import random\n","from tqdm import tqdm\n","from PIL import Image\n","import glob\n","import pickle\n","from time import time\n","import tensorflow as tf\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","# from keras.preprocessing import image\n","\n","NO_OF_IMAGES = 15000 # number of images to load (NOTE: it is different from training size since there are multiple captions per image)\n","MAX_LEN = 20    # max length of caption\n","EMBED_SIZE = 200\n","TRAIN_SIZE = .8\n","TEST_SIZE = .2"]},{"cell_type":"markdown","metadata":{"id":"OnVrNV4o6rXy"},"source":["# process images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6894,"status":"ok","timestamp":1651514574271,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"yvExnKL01E2l","outputId":"fb86cf3e-28a7-4d40-b964-a78af44abfa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["total images:15000\n","dict_keys(['info', 'licenses', 'images', 'annotations'])\n"]}],"source":["# images_list = os.listdir(os.path.join(data_root, 'train2017'))    # list all the images in folder.\n","# pickle.dump(images_list, open(os.path.join(data_root, 'dump', 'images_list'), 'wb'))  # store images file names so that we don't need to retrive again.\n","images_list = pickle.load(open(os.path.join(data_root, 'dump', 'images_list'), 'rb'))   # load the images finle names from dump.\n","\n","images_list = images_list[:NO_OF_IMAGES]\n","print(f'total images:{len(images_list)}')\n","image_dict = {image: [] for image in images_list}       # create dict to store image and it's captions in a list.\n","\n","data = json.loads(open(os.path.join(data_root, 'captions_train2017.json'), 'r').read())\n","print(data.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-z4eh8voAznS"},"outputs":[],"source":["images = data['images']\n","annotations = data['annotations']\n","\n","image2id, id2image = {}, {} # every image file has unique id and unique file name create dict of file name -> id and id -> file name.\n","\n","images_set = set(images_list)\n","for image in images:\n","    if image['file_name'] in images_set:\n","        image2id[image['file_name']] = image['id']\n","        id2image[image['id']] = image['file_name']\n","\n","captions_404 = [] # captions which does not have any image (since we didn't consider all the images) use these caption for -ve example generation.\n","\n","for caption in annotations:\n","    if id2image.get(caption['image_id']) is None:\n","        captions_404.append(caption['caption'])\n","    else:\n","        image_dict[id2image[caption['image_id']]].append(caption['caption'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ho6dVHLQLFG"},"outputs":[],"source":["# load images from dump to save time.\n","# image_loaded = {}\n","# pickle.dump(image_loaded, open(os.path.join(data_root, 'dump', 'images_loaded_299'), mode='wb'))  # use it for first time.\n","image_loaded = pickle.load(open(os.path.join(data_root, 'dump', 'images_loaded_299'), mode='rb'))\n","def load_image(file_name):\n","    img = image_loaded.get(file_name)   # retrive image if it is already loaded from disk.\n","    if img is None:\n","        img = tf.keras.preprocessing.image.load_img(os.path.join(data_root, 'train2017',file_name), target_size=(299, 299)) # load new image from disk\n","        image_loaded[file_name] = img   # save image to dict and later dump this dict so that we can retrive dict in next runs of program.\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1103073,"status":"ok","timestamp":1651515749972,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"FEp8EiugO_ht","outputId":"2c465e3d-1f15-455c-f4f1-fffb41f0e197"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n","96116736/96112376 [==============================] - 1s 0us/step\n","96124928/96112376 [==============================] - 1s 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15000/15000 [17:57<00:00, 13.93it/s]\n"]}],"source":["inceptionV3_model = InceptionV3(weights='imagenet') # InceptionV3 is used to extract the features from image(you can use your own CNN but it is trained on huge dataset)\n","my_inceptionV3_model = tf.keras.models.Model(inputs=inceptionV3_model.input, outputs=inceptionV3_model.layers[-2].output)   # take second last layer as output layer.\n","\n","def preprocess_image(file_name):\n","    img = load_image(file_name)\n","    img = tf.keras.preprocessing.image.img_to_array(img)    # convert image from PIL form to array of (width, height, channels) shape.\n","    img = np.expand_dims(img, axis=0) # inceptionV3 expects input as (1, width, height, channels)\n","    img = preprocess_input(img)\n","    return img\n","\n","def extract_features(file_name):\n","    img = preprocess_image(file_name)\n","    feature_vec = my_inceptionV3_model.predict(img) # extract the feature vector from image it is of shape (1, 2048)\n","    feature_vec = feature_vec.reshape(feature_vec.shape[1], )   # reshape to (2048,) 1D vector\n","    return feature_vec\n","\n","\n","\n","image_feature = {}  # map file name to feature vector\n","for file_name in tqdm(images_list):\n","    image_feature[file_name] = extract_features(file_name)\n","\n","# now dump the newly loaded images to file along with already loaded images present in dict\n","pickle.dump(image_loaded, open(os.path.join(data_root, 'dump', 'images_loaded_299'), 'wb'))"]},{"cell_type":"markdown","metadata":{"id":"n3G2iKZW6yex"},"source":["#process captions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOLWdsKRVJG3"},"outputs":[],"source":["def remove_punctuations(list):\n","    processed_list = []\n","    filters='''!'\"“”#$%&()⟨⟩*+,-–—./:;<=>?@[\\\\]^_`’{|}~\\t\\n'''\n","    for caption in list:\n","        caption = caption.lower()\n","        for char in filters:\n","            caption = caption.replace(char, ' ')\n","            caption = ' '.join(caption.split())\n","        processed_list.append(caption)\n","    return processed_list\n","\n","def _tokenizer(train_captions):\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        filters='''!'\"“”#$%&()⟨⟩*+,-–—./:;<=>?@[\\\\]^_`’{|}~\\t\\n''',\n","        lower=True\n","    )\n","    tokenizer.fit_on_texts(train_captions)\n","    return tokenizer\n","\n","def encoder(sents, tokenizer):\n","    encoded_sents = tokenizer.texts_to_sequences(sents)\n","    padded_sents = tf.keras.preprocessing.sequence.pad_sequences(\n","        sequences=encoded_sents,\n","        maxlen=MAX_LEN,\n","        padding='post'\n","    )\n","    return padded_sents\n","\n","for key, captions in image_dict.items():\n","    image_dict[key] = remove_punctuations(image_dict[key])  # process the captions.\n","    \n","captions_404 = remove_punctuations(captions_404)"]},{"cell_type":"markdown","metadata":{"id":"5do_RhYkPKul"},"source":["#split data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5968,"status":"ok","timestamp":1651515779108,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"Y_G2QNTVPJeT","outputId":"fa24d8dd-de4b-46b2-c429-554114a7210e"},"outputs":[{"output_type":"stream","name":"stdout","text":["train_size:12000, test_size:3000\n"]}],"source":["data = list(image_dict.items()) # image_dict has image file name as key and list of captions as value.\n","random.shuffle(data)\n","length = len(data)\n","\n","train_data = data[ : math.ceil(TRAIN_SIZE*length)]\n","test_data = data[math.ceil((TRAIN_SIZE)*length) : ]\n","print(f'train_size:{len(train_data)}, test_size:{len(test_data)}')\n","\n","def getDataDict(data):\n","    data_dict = {}\n","    for image, list in data:\n","        data_dict[image] = list\n","    return data_dict\n","\n","train_data = getDataDict(train_data)    # train data is list of tuples convert it to dictionary of {file name: list of caption}\n","test_data = getDataDict(test_data)\n","# print(f'{list(train_data.items())[:10]}, \\n{list(val_data.items())[:10]}, \\n{list(test_data.items())[:10]}')\n","\n","train_captions = [] # take all the captions in training data to be used in tokenizer (we can not use test data in tokenizer)\n","for _, captions in train_data.items():\n","    train_captions.extend(captions)\n","\n","tokenizer = _tokenizer(train_captions)\n","pickle.dump(tokenizer, open(os.path.join(data_root, 'dump', 'tokenizer'), 'wb'))\n","\n","train_x, train_y = [], []\n","test_x, test_y = [], []\n","\n","\n","#TODO: instead of taking 5 captions of same image better to keep 5 different image with one caption.\n","def prepare_dataset(data_dict):\n","    data_x, data_y = [], []\n","    for key, captions in data_dict.items():\n","        count = 1 #len(captions)\n","        captions = encoder(captions, tokenizer)\n","        for caption in captions[:1]:    # taking +ve examples\n","            data_x.append([image_feature[key], caption])\n","            data_y.append(1)\n","\n","        captions = encoder(random.sample(captions_404, count), tokenizer)\n","        for caption in captions:  # taking same number of -ve example\n","            data_x.append([image_feature[key], caption])\n","            data_y.append(0)\n","    return data_x, data_y\n","\n","train_x, train_y = prepare_dataset(train_data)  # create a list of list with contains [[image_feature vector, single caption], ...]\n","test_x, test_y = prepare_dataset(test_data)\n","\n","# print(f'train_x:{len(train_x)}, val_x:{len(val_x)}, test_x:{len(test_x)}')"]},{"cell_type":"markdown","metadata":{"id":"8ZfHuecNux9Z"},"source":["#model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgkkepdQ-_EZ"},"outputs":[],"source":["\"\"\" Downloading and extracting word vectors\n","\"\"\"\n","# !wget http://nlp.stanford.edu/data/glove.6B.zip -P '/content/drive/My Drive/IITB Courses/CS 772/Project/data/glove'\n","# import zipfile\n","# with zipfile.ZipFile('/content/drive/My Drive/IITB Courses/CS 772/Project/data/glove/'+'glove.6B.zip', 'r') as zip_ref:\n","#     zip_ref.extractall('/content/drive/My Drive/IITB Courses/CS 772/Project/data/glove')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23051,"status":"ok","timestamp":1651515802078,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"tq_O7llPLiq2","outputId":"ee1b5ce4-cd0c-4e51-ea3c-d46cc5b38ed3"},"outputs":[{"output_type":"stream","name":"stdout","text":["size of vocab 10414\n","(10414, 200)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 20)]         0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 2048)]       0           []                               \n","                                                                                                  \n"," embedding (Embedding)          (None, 20, 200)      2082800     ['input_3[0][0]']                \n","                                                                                                  \n"," dropout (Dropout)              (None, 2048)         0           ['input_2[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 20, 200)      0           ['embedding[0][0]']              \n","                                                                                                  \n"," dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n","                                                                                                  \n"," lstm (LSTM)                    (None, 256)          467968      ['dropout_1[0][0]']              \n","                                                                                                  \n"," concatenate_2 (Concatenate)    (None, 512)          0           ['dense[0][0]',                  \n","                                                                  'lstm[0][0]']                   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 256)          131328      ['concatenate_2[0][0]']          \n","                                                                                                  \n"," dense_2 (Dense)                (None, 1)            257         ['dense_1[0][0]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 3,206,897\n","Trainable params: 1,124,097\n","Non-trainable params: 2,082,800\n","__________________________________________________________________________________________________\n"]}],"source":["vocab_size = len(tokenizer.word_index)+1    # index of word starts from 1 hence vocab size is more because we have to include 0th index.\n","print('size of vocab', vocab_size)\n","\n","embeddings = {}\n","with open(os.path.join(data_root, 'glove', 'glove.6B.'+str(EMBED_SIZE)+'d.txt'), mode='r', encoding='utf8') as f:\n","    for line in f:\n","        line = line.split()\n","        embeddings[line[0]] = np.asarray(line[1:], dtype='float32')\n","\n","embed_init = np.zeros((vocab_size, EMBED_SIZE))\n","\n","for word, idx in tokenizer.word_index.items():\n","    vec = embeddings.get(word)\n","    if vec is not None:\n","        embed_init[idx] = vec\n","\n","embedding_layer = tf.keras.layers.Embedding(\n","    input_dim=vocab_size,\n","    output_dim=EMBED_SIZE,\n","    embeddings_initializer=tf.keras.initializers.Constant(embed_init),\n","    trainable=False,\n","    mask_zero=True\n",")\n","print(embed_init.shape)\n","\n","img_input = tf.keras.Input(shape=(2048,))   # input of image feature vector\n","img1 = tf.keras.layers.Dropout(rate=.5)(img_input)\n","img2 = tf.keras.layers.Dense(units=256, activation='relu')(img1)\n","\n","cap_input = tf.keras.Input(shape=(MAX_LEN,))    # input of caption\n","cap1 = embedding_layer(cap_input)   # returns matrix of shape (number of words, vector size)\n","cap2 = tf.keras.layers.Dropout(rate=.5)(cap1)\n","cap3 = tf.keras.layers.LSTM(units=256)(cap2)    # after applying entire input sequence it returns the last hidden state of LSTM.\n","\n","combine = tf.keras.layers.concatenate([img2, cap3])\n","combine1 = tf.keras.layers.Dense(units=256, activation='relu')(combine)\n","\n","output = tf.keras.layers.Dense(units=1, activation='sigmoid')(combine1) # output neuron since we have to output single probability sigmoid is used, in case of prob distribution softmax is used.\n","model = tf.keras.models.Model(inputs=[img_input, cap_input], outputs=output) # model has two input both are in a list and one output. we need to supply the training data in the same format to model.fit() like input will be list of two list/tensor output will be single list\n","\n","model.summary()\n","tf.keras.utils.plot_model(model, to_file='/content/drive/My Drive/IITB Courses/CS 772/Project/model_inception.png')   # to output the model diagram.\n","\n","model.compile(\n","    loss=tf.keras.losses.BinaryCrossentropy(), # since sigmoid is used, BinaryCrossentropy() is used with sigmod in case of single output.\n","    optimizer=tf.keras.optimizers.Adam(),\n","    metrics=['accuracy']\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frXupZHZonyl","executionInfo":{"status":"ok","timestamp":1651516019933,"user_tz":-330,"elapsed":217907,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"}},"outputId":"69d24a91-cc2f-4653-d470-c9cc4c23ba2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","600/600 [==============================] - 13s 11ms/step - loss: 0.6292 - accuracy: 0.6183 - val_loss: 0.5000 - val_accuracy: 0.7481\n","Epoch 2/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.4734 - accuracy: 0.7732 - val_loss: 0.4290 - val_accuracy: 0.7994\n","Epoch 3/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.3892 - accuracy: 0.8294 - val_loss: 0.3514 - val_accuracy: 0.8542\n","Epoch 4/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.3389 - accuracy: 0.8545 - val_loss: 0.3353 - val_accuracy: 0.8629\n","Epoch 5/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.2971 - accuracy: 0.8782 - val_loss: 0.3032 - val_accuracy: 0.8785\n","Epoch 6/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.2685 - accuracy: 0.8914 - val_loss: 0.2850 - val_accuracy: 0.8854\n","Epoch 7/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.2436 - accuracy: 0.9031 - val_loss: 0.2835 - val_accuracy: 0.8904\n","Epoch 8/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.2264 - accuracy: 0.9108 - val_loss: 0.2807 - val_accuracy: 0.8942\n","Epoch 9/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.2090 - accuracy: 0.9188 - val_loss: 0.2708 - val_accuracy: 0.9000\n","Epoch 10/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1886 - accuracy: 0.9249 - val_loss: 0.2782 - val_accuracy: 0.8992\n","Epoch 11/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1733 - accuracy: 0.9329 - val_loss: 0.3056 - val_accuracy: 0.8917\n","Epoch 12/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1660 - accuracy: 0.9368 - val_loss: 0.2740 - val_accuracy: 0.9060\n","Epoch 13/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1478 - accuracy: 0.9413 - val_loss: 0.3072 - val_accuracy: 0.8983\n","Epoch 14/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1370 - accuracy: 0.9472 - val_loss: 0.3006 - val_accuracy: 0.9023\n","Epoch 15/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1284 - accuracy: 0.9515 - val_loss: 0.3097 - val_accuracy: 0.9019\n","Epoch 16/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1247 - accuracy: 0.9510 - val_loss: 0.3206 - val_accuracy: 0.9013\n","Epoch 17/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1119 - accuracy: 0.9568 - val_loss: 0.3363 - val_accuracy: 0.8977\n","Epoch 18/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1061 - accuracy: 0.9600 - val_loss: 0.3401 - val_accuracy: 0.8971\n","Epoch 19/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.1001 - accuracy: 0.9618 - val_loss: 0.3270 - val_accuracy: 0.9050\n","Epoch 20/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0945 - accuracy: 0.9643 - val_loss: 0.3577 - val_accuracy: 0.8950\n","Epoch 21/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0900 - accuracy: 0.9664 - val_loss: 0.3654 - val_accuracy: 0.9008\n","Epoch 22/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0858 - accuracy: 0.9670 - val_loss: 0.3670 - val_accuracy: 0.9046\n","Epoch 23/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0819 - accuracy: 0.9689 - val_loss: 0.3907 - val_accuracy: 0.9027\n","Epoch 24/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0840 - accuracy: 0.9681 - val_loss: 0.3982 - val_accuracy: 0.8979\n","Epoch 25/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0733 - accuracy: 0.9730 - val_loss: 0.3656 - val_accuracy: 0.9048\n","Epoch 26/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0699 - accuracy: 0.9745 - val_loss: 0.3874 - val_accuracy: 0.9033\n","Epoch 27/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0720 - accuracy: 0.9736 - val_loss: 0.4066 - val_accuracy: 0.9008\n","Epoch 28/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0605 - accuracy: 0.9770 - val_loss: 0.3996 - val_accuracy: 0.9017\n","Epoch 29/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0657 - accuracy: 0.9747 - val_loss: 0.4380 - val_accuracy: 0.8960\n","Epoch 30/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0632 - accuracy: 0.9763 - val_loss: 0.4231 - val_accuracy: 0.9010\n","Epoch 31/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.0612 - accuracy: 0.9767 - val_loss: 0.4514 - val_accuracy: 0.8973\n","Epoch 32/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.0583 - accuracy: 0.9786 - val_loss: 0.4615 - val_accuracy: 0.8904\n","Epoch 33/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.0599 - accuracy: 0.9774 - val_loss: 0.4400 - val_accuracy: 0.9023\n","Epoch 34/40\n","600/600 [==============================] - 5s 8ms/step - loss: 0.0603 - accuracy: 0.9765 - val_loss: 0.4647 - val_accuracy: 0.8963\n","Epoch 35/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0537 - accuracy: 0.9802 - val_loss: 0.4482 - val_accuracy: 0.8921\n","Epoch 36/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0517 - accuracy: 0.9810 - val_loss: 0.4359 - val_accuracy: 0.8958\n","Epoch 37/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0540 - accuracy: 0.9801 - val_loss: 0.4765 - val_accuracy: 0.8938\n","Epoch 38/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0511 - accuracy: 0.9825 - val_loss: 0.4882 - val_accuracy: 0.8931\n","Epoch 39/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0557 - accuracy: 0.9810 - val_loss: 0.4543 - val_accuracy: 0.8950\n","Epoch 40/40\n","600/600 [==============================] - 4s 7ms/step - loss: 0.0452 - accuracy: 0.9835 - val_loss: 0.4836 - val_accuracy: 0.8988\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/IITB Courses/CS 772/Project/data/dump/model/assets\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Assets written to: /content/drive/My Drive/IITB Courses/CS 772/Project/data/dump/model/assets\n","WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f21e7019890> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"]}],"source":["train_x_img, train_x_cap = [], []   # we have training data into same list but model expects two input hence we need to separate the two input data.\n","\n","for img, cap in train_x:\n","    train_x_img.append(img)\n","    train_x_cap.append(cap)\n","\n","history = model.fit(\n","    x=[tf.constant(train_x_img), tf.constant(train_x_cap)],     # model was giving error so I converted to tensors using tf.constant().\n","    y=tf.constant(train_y),\n","    batch_size=32,\n","    epochs=40,\n","    validation_split=0.2\n","    )\n","model.save(os.path.join(data_root, 'dump', 'model'))"]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support\n","import sklearn.metrics\n","\n","test_x_img, test_x_cap = [], []\n","\n","for img, cap in test_x:\n","    test_x_img.append(img)\n","    test_x_cap.append(cap)\n","\n","predictions = model.predict([tf.constant(test_x_img), tf.constant(test_x_cap)])\n","predictions = tf.squeeze(predictions)   # prediction is of shape (number of examples, 1) hence I squeezed to make it 1d.\n","\n","loss = tf.keras.losses.BinaryCrossentropy()\n","print(f'test loss is:{loss(test_y, predictions).numpy()}')\n","\n","pred_y = []\n","for val in predictions: # since we are going to predict non-consistent caption if prediction is < .5 hence we are making it 0 and 1 otherwise now see what is error.\n","    if val < .5:\n","        pred_y.append(0)\n","    else:\n","        pred_y.append(1)\n","prec, rec, fscore, support = precision_recall_fscore_support(test_y, pred_y, average='weighted')\n","\n","print(f'prec:{prec}, recall:{rec}, fsocre:{fscore}')\n","\n","correct = 0\n","for x,y in zip(test_y, pred_y):\n","    if x == y:\n","        correct += 1\n","\n","print(f'accuracy:{correct/len(test_y)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCut5lFMkyCU","executionInfo":{"status":"ok","timestamp":1651516027069,"user_tz":-330,"elapsed":7186,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"}},"outputId":"1ef55d7a-c77a-41c5-e3b9-d864c8291737"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test loss is:0.4255835711956024\n","prec:0.9049387949622608, recall:0.9041666666666667, fsocre:0.9041209616617523\n","accuracy:0.9041666666666667\n"]}]},{"cell_type":"code","source":["test_0, pred_0 = [], []\n","test_1, pred_1 = [], []\n","\n","for x,y in zip(test_y, pred_y):\n","    if x == 1:\n","        test_1.append(x)\n","        pred_1.append(y)\n","    else:\n","        test_0.append(x)\n","        pred_0.append(y)\n","\n","prec, rec, fscore, support = precision_recall_fscore_support(test_0, pred_0, average='weighted')\n","\n","print(f'prec:{prec}, recall:{rec}, fsocre:{fscore}')\n","\n","prec, rec, fscore, support = precision_recall_fscore_support(test_1, pred_1, average='weighted')\n","\n","print(f'prec:{prec}, recall:{rec}, fsocre:{fscore}')"],"metadata":{"id":"S6UBUTm2dnDk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"viLe4rmuCGuF"},"source":["#testing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19280,"status":"ok","timestamp":1651516046257,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"btOtcLPaCInl","outputId":"d4498cd1-4d50-4c6c-8dd9-9eb2935d2b35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data_root = '/content/drive/My Drive/IITB Courses/CS 772/Project/data'\n","\n","import pickle\n","import os\n","import tensorflow as tf\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","import numpy as np\n","MAX_LEN = 20\n","\n","tokenizer = pickle.load(open(os.path.join(data_root, 'dump', 'tokenizer'), 'rb'))\n","model = tf.keras.models.load_model(os.path.join(data_root, 'dump', 'model'))\n","\n","def encoder(sents, tokenizer):\n","    encoded_sents = tokenizer.texts_to_sequences(sents)\n","    padded_sents = tf.keras.preprocessing.sequence.pad_sequences(\n","        sequences=encoded_sents,\n","        maxlen=MAX_LEN,\n","        padding='post'\n","    )\n","    return padded_sents\n","\n","inceptionV3_model = InceptionV3(weights='imagenet')\n","my_inceptionV3_model = tf.keras.models.Model(inputs=inceptionV3_model.input, outputs=inceptionV3_model.layers[-2].output)\n","\n","def preprocess_image(file_name):\n","    img = tf.keras.preprocessing.image.load_img(file_name, target_size=(299, 299))\n","    img = tf.keras.preprocessing.image.img_to_array(img)\n","    img = np.expand_dims(img, axis=0) # inceptionV3 expects input as (1, width, height, channels)\n","    img = preprocess_input(img)\n","    return img\n","\n","\n","def extract_features(file_name):\n","    img = preprocess_image(file_name)\n","    feature_vec = my_inceptionV3_model.predict(img)\n","    feature_vec = feature_vec.reshape(feature_vec.shape[1], )\n","    return feature_vec\n","\n","# upload image, take its name.\n","# input captions\n","# convert image to feature vector,\n","\n","# use model.predict with expand_dim since single input is supplied."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"executionInfo":{"elapsed":9640,"status":"ok","timestamp":1651430511357,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"hMpPL-uXcIU2","outputId":"768d339d-3a7a-43f7-c83a-5875b396fe6b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-4c1e7288-e4b0-462d-883d-f68b45bee633\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4c1e7288-e4b0-462d-883d-f68b45bee633\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving COCO_test2014_000000000128.jpg to COCO_test2014_000000000128.jpg\n","['COCO_test2014_000000000128.jpg']\n"]}],"source":["def upload_file():\n","    from google.colab import files\n","    uploaded = files.upload()\n","    for k, v in uploaded.items():\n","        open(k, 'wb').write(v)\n","    return list(uploaded.keys())\n","print(upload_file()) # printing the file name."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":990,"status":"ok","timestamp":1651432060892,"user":{"displayName":"Deepak Baghel","userId":"02340579537340729659"},"user_tz":-330},"id":"1aiAfdWZdcew","outputId":"62f90815-3d23-431b-915e-36bce233894e"},"outputs":[{"output_type":"stream","name":"stdout","text":["similarity is:[0.0026902]\n"]}],"source":["caption = 'There is an elephant'\n","feature_vec = extract_features('COCO_test2014_000000000128.jpg')\n","# print(type(feature_vec), feature_vec)\n","cap = encoder([caption], tokenizer)[0]\n","# print(type(cap), cap)\n","print(f'similarity is:{model.predict([tf.constant([feature_vec]), tf.constant([cap])])[0]}') # since model.predict expects batch input hence expanding dimension by enquoting feature vector and caption inside list separately. NOTE: outer list is to say there are two inputs inner list is expanding the  dim."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Image Captioning using InceptionV3.ipynb","provenance":[{"file_id":"1aYt9OT0NuqaCmOY1c7-03EFzDiPuVo_2","timestamp":1649588608043}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}